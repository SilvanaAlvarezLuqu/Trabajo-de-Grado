{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afc00c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer, EncoderNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss, MAPE, RMSE, MAE, NormalDistributionLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ece5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ae0741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio donde se encuentra la base de datos con los índices x serie\n",
    "os.chdir('C:\\\\Users\\\\ALVARLX23\\\\OneDrive - Abbott\\\\Documents\\\\Proyecto\\\\Redes Neuronales')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb98ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer datos y convertir variables en categorias\n",
    "DATA= pd.read_excel('data.xlsx') \n",
    "DATA['timeseries']=pd.Categorical(DATA.timeseries) \n",
    "DATA['brand']=pd.Categorical(DATA.brand)\n",
    "DATA['channel']=pd.Categorical(DATA.channel)\n",
    "DATA['units'] = DATA['units'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6971db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir índice de tiempo\n",
    "DATA[\"time_idx\"] = DATA[\"YearMonth\"].dt.year * 12 + DATA[\"YearMonth\"].dt.month\n",
    "DATA[\"time_idx\"] -= DATA[\"time_idx\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d9a66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ver el tamaño y la clase de las variables y el data frame\n",
    "print(DATA.shape)\n",
    "print(DATA.dtypes)\n",
    "type(DATA)\n",
    "\n",
    "# Ver las primeras observaciones\n",
    "DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec23d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar variables \n",
    "DATA[\"month\"] = DATA.YearMonth.dt.month.astype(str).astype(\"category\")  # Las categorias tienen que ser strings\n",
    "DATA[\"avg_volume_by_sku\"] = DATA.groupby([\"time_idx\", \"sku\"], observed=True).units.transform(\"mean\") # la misma fecha x sku (promedio entre los dos canales)\n",
    "DATA[\"avg_volume_by_channel\"] = DATA.groupby([\"time_idx\", \"channel\"], observed=True).units.transform(\"mean\") # todos los de la misma fecha x canal\n",
    "DATA[\"avg_volume_by_brand\"] = DATA.groupby([\"time_idx\", \"brand\"], observed=True).units.transform(\"mean\") # todos los de la misma fecha x canal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf31d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meses de prueba + 28 Fcst\n",
    "max_prediction_length = round(DATA.shape[0]/np.unique(DATA.timeseries).shape[0]*0.1) + (28 - round(DATA.shape[0]/np.unique(DATA.timeseries).shape[0]*0.1))\n",
    "# Meses de entrenamiento (90% datos = que con ARIMA)\n",
    "max_encoder_length = round(DATA.shape[0]/np.unique(DATA.timeseries).shape[0]*0.9)\n",
    "training_cutoff = DATA[\"time_idx\"].max() - (max_prediction_length - (28 - round(DATA.shape[0]/np.unique(DATA.timeseries).shape[0]*0.1)))\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    DATA[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"units\",\n",
    "    group_ids=[\"timeseries\", \"channel\"],\n",
    "    min_encoder_length=max_encoder_length // 2,  \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"channel\", \"sku\", \"brand\"],\n",
    "    static_reals=[\"SAP\"],\n",
    "    time_varying_known_categoricals=[\"month\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"units\",\n",
    "        \"avg_volume_by_channel\",\n",
    "        \"avg_volume_by_sku\",\n",
    "        \"avg_volume_by_brand\",\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"timeseries\",\"channel\"], transformation=\"softplus\" #\n",
    "    ),  # Usa softplus para normalizar por grupos\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    allow_missing_timesteps=True\n",
    "    #add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# crear conjunto de validación (predict=True) que significa predecir los max_prediction_length puntos en el tiempo\n",
    "# para cada serie\n",
    "validation = TimeSeriesDataSet.from_dataset(training, DATA, predict=True, stop_randomization=True)\n",
    "\n",
    "# crear dataloaders para el modelo\n",
    "batch_size = 32  # En general 32 y probar bajando y subiendo por potencias de 2\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=4)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed1bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcular MAE base. las predicciónes quedan como el último valor disponible (esto para tener un punto de referencia)\n",
    "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
    "baseline_predictions = Baseline().predict(val_dataloader)\n",
    "(actuals[:, 0:7] - baseline_predictions[:,0:7]).abs().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49629807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurar la red y el entrenador\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    gpus=0,\n",
    "    # fijar el gradiente es un hiperparámetro y es importante para evitar la divergencia\n",
    "    # del gradiente para una red neuronal recurrente\n",
    "    gradient_clip_val=0.2,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=8,  \n",
    "    attention_head_size=1,\n",
    "    dropout=0.2,  # entre 0.1 y 0.3 son buenos valores\n",
    "    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "    output_size=1,  \n",
    "    loss=QuantileLoss([0.5]), # utilizar la mediana como predictor\n",
    "    # reducir la tasa de aprendizaje si no hay mejora en la pérdida de validación después de x epochs\n",
    "    reduce_on_plateau_patience=2,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a38dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar la tasa de aprendizaje óptima (suele dar una demasiado pequeña)\n",
    "res = trainer.tuner.lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96818323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar la red y el entrenador teniendo en cuenta la tasa de aprendizaje\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log de la tasa de aprendizaje\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    gpus=0,\n",
    "    weights_summary=\"top\",\n",
    "    gradient_clip_val=0.2,\n",
    "    limit_train_batches=32, \n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=16,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.15,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.2,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=1,  \n",
    "    loss=QuantileLoss([0.5]),\n",
    "    log_interval=10,  \n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee461ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar red (este se demora en correr dado que es donde se hace el ajuste del modelo)\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56e2907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el mejor modelo según la pérdida de validación\n",
    "# (como se usó early stopping no necesariamente es el último epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLICA QUE TIPI DE PARAMETROS Y CUANTOS PARAMETROS AJUSTO EL MODELO\n",
    "best_tft.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f75ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcular error medio absoluto \n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = best_tft.predict(val_dataloader)\n",
    "(actuals[:,0:7] - predictions[:,0:7]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab82aee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(training.to_dataloader(batch_size=32)))\n",
    "y[0]  # Lista de valores target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1cf017",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = best_tft(x)\n",
    "out\n",
    "\n",
    "best_tft.loss(out[\"prediction\"], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57b972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw predictions: diccionario con todo tipo de info (ej. cuantiles)\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badba80b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Solo es información de que variables tuvo en cuenta\n",
    "interpretation = best_tft.interpret_output(raw_predictions, reduction=\"sum\")\n",
    "best_tft.plot_interpretation(interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dbccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# realizar las predicciones con el mejor modelo para el conjunto de validación\n",
    "predictions = best_tft.predict(val_dataloader)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd8751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertirpredicciones de un tensor a un data frame\n",
    "y_pred=pd.DataFrame(predictions[:,0:7].numpy()) \n",
    "y_pred.index = np.unique(DATA['timeseries'])\n",
    "y_pred.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7779f",
   "metadata": {},
   "source": [
    "### Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf482af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prueba = predictions[:,0:7]\n",
    "actual_prueba = actuals[:, 0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd9103",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RMSE\n",
    "a = ((actual_prueba - pred_prueba)**2).mean(axis=1)\n",
    "rmse = []\n",
    "\n",
    "for i in a:\n",
    "    rmse.append(math.sqrt(i))\n",
    "print(rmse)\n",
    "\n",
    "# MAPE\n",
    "mape= ((actual_prueba - pred_prueba).abs()/actual_prueba).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad187ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(DATA['sku']).shape # revisar cuando vuelva a correrlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2828ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adecuación para que salga el Data Frame de métricas\n",
    "metricas = pd.DataFrame({\"time Serie\": np.unique(DATA['timeseries']), \"mape\": mape.numpy(),\"rmse\": np.asarray(rmse)})\n",
    "metricas.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d3404c",
   "metadata": {},
   "source": [
    "### Predicción en nuevos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aedfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_data = DATA[lambda x: x.time_idx > x.time_idx.max() - max_encoder_length]\n",
    "\n",
    "# Seleccionar los últimos puntos conocidos y creamos el decoder a partir de allí repitiendo incrementando la fecha\n",
    "last_data = DATA[lambda x: x.time_idx == x.time_idx.max()]\n",
    "\n",
    "decoder_data = pd.concat(\n",
    "    [last_data.assign(YearMonth=lambda x: x.YearMonth + pd.offsets.MonthBegin(i)) for i in range(1, max_prediction_length + 1)],\n",
    "    ignore_index=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTA SE VA A CORTAR, PERO POR AHORA NO HASTA ESTAR SEGURA AL CORRERLO AGAIN\n",
    "\n",
    "print(min(encoder_data.YearMonth),\n",
    "      max(encoder_data.YearMonth),\n",
    "      min(decoder_data.YearMonth),\n",
    "      max(decoder_data.YearMonth),\n",
    "      len(encoder_data), len(decoder_data), sep = \"\\t\"\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad2818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir índice de tiempo consistente con la data\n",
    "decoder_data[\"time_idx\"] = decoder_data[\"YearMonth\"].dt.year * 12 + decoder_data[\"YearMonth\"].dt.month\n",
    "decoder_data[\"time_idx\"] += encoder_data[\"time_idx\"].max() + 1 - decoder_data[\"time_idx\"].min()\n",
    "\n",
    "# ajustar características de tiempo\n",
    "decoder_data[\"month\"] = decoder_data.YearMonth.dt.month.astype(str).astype(\"category\")  # categories have be strings\n",
    "\n",
    "# unir enconder y decoder\n",
    "new_prediction_data = pd.concat([encoder_data, decoder_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed9f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_raw_predictions, new_x = best_tft.predict(new_prediction_data, mode=\"raw\", return_x=True)\n",
    "\n",
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(new_x, new_raw_predictions, idx=idx, show_future_observed=False);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c99d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 155 -> PEDIASURE CLINICAL RPB 220ML \n",
    "# 121 -> ENSURE BASE RPB VAINILLA\n",
    "best_tft.plot_prediction(new_x, new_raw_predictions, idx=123, show_future_observed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e509e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# realizar las predicciones con el mejor modelo para el conjunto de validación\n",
    "predictions_new = best_tft.predict(new_prediction_data)\n",
    "predictions_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertirpredicciones de un tensor a un data frame\n",
    "y_new=pd.DataFrame(predictions_new.numpy()) \n",
    "y_new.index = np.unique(decoder_data['timeseries'])\n",
    "y_new.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a excel(analogo a R)\n",
    "writer = pd.ExcelWriter('C:\\\\Users\\\\ALVARLX23\\\\OneDrive - Abbott\\\\Documents\\\\Proyecto\\\\Redes Neuronales\\\\salida_redes.xlsx')\n",
    "y_pred.to_excel(writer, sheet_name=\"estimación\", index=True)\n",
    "metricas.to_excel(writer, sheet_name=\"metricas\", index=False)\n",
    "y_new.to_excel(writer, sheet_name=\"28 meses\", index=True)\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3592f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
